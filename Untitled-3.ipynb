{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7238a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/sprint2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afe78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: e8bf35bf-65cd-40e1-8548-51e68061ecaf)')' thrown while requesting HEAD https://huggingface.co/datasets/wikitext/resolve/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 6650, Val texts: 350\n"
     ]
    }
   ],
   "source": [
    "# импортируем библиотеки, которые пригодятся для задачи\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# функция для \"чистки\" текстов\n",
    "def clean_string(text):\n",
    "    # приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # удаление всего, кроме латинских букв, цифр и пробелов\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # удаление дублирующихся пробелов, удаление пробелов по краям\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# загружаем датасет WikiText-2\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# длины последовательностей в датасете\n",
    "# seq_len = 7 => 3 токена до <MASK> + токен <MASK> + 3 токена после\n",
    "seq_len = 7\n",
    "\n",
    "# удаляем слишком короткие тексты\n",
    "texts = [line for line in dataset[\"text\"] if len(line.split()) >= seq_len]\n",
    "\n",
    "# \"чистим\" тексты\n",
    "cleaned_texts = list(map(clean_string, texts))\n",
    "\n",
    "# для упрощения используем только max_texts_count текстов\n",
    "max_texts_count = 7000\n",
    "\n",
    "# разбиение на тренировочную и валидационную выборки\n",
    "val_size = 0.05\n",
    "\n",
    "train_texts, val_texts = train_test_split(cleaned_texts[:max_texts_count], test_size=val_size, random_state=42)\n",
    "print(f\"Train texts: {len(train_texts)}, Val texts: {len(val_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b94688c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс датасета\n",
    "class MaskedBertDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=7):\n",
    "        # self.samples - список пар (x, y)\n",
    "        # x - токенизированный текст с пропущенным токеном\n",
    "        # y - пропущенный токен\n",
    "        self.samples = []\n",
    "        for line in texts:\n",
    "            token_ids = tokenizer(line, truncation=True)['input_ids'] # токенизируйте строку line\n",
    "            # если строка слишком короткая, то пропускаем её\n",
    "            if len(token_ids) < seq_len:\n",
    "                continue\n",
    "            # проходимся по всем токенам в последовательности\n",
    "            for i in range(1, len(token_ids) - 1):\n",
    "                '''\n",
    "                context - список из seq_len // 2 токенов до i-го токена, токена tokenizer.mask_token_id, и seq_len // 2 токенов после i-го токена\n",
    "                '''\n",
    "                context = token_ids[(i-seq_len)*(i > seq_len):i] + [tokenizer.mask_token_id] + token_ids[i+1: i+seq_len+1] # соберите контекст вокруг i-го токена\n",
    "                # если контекст слишком короткий, то пропускаем его\n",
    "                if len(context) < seq_len:\n",
    "                    continue\n",
    "                target = token_ids[i] # возьмите i-ый токен последовательности\n",
    "                self.samples.append((context, target))\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.samples) # верните размер датасета\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y =  self.samples[idx] # получите контекст и таргет для элемента с индексом idx\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# загружаем токенизатор\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# тренировочный и валидационный датасеты\n",
    "train_dataset = MaskedBertDataset(train_texts, tokenizer, seq_len=seq_len)\n",
    "val_dataset = MaskedBertDataset(val_texts, tokenizer, seq_len=seq_len)\n",
    "\n",
    "# даталоадеры\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0a44d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[10, 11, 12, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "l = list(range(10, 20))\n",
    "seq_len = 3\n",
    "i = 3\n",
    "print(l)\n",
    "print(l[(i-seq_len)*(i > seq_len):i] + l[i+1: i+seq_len+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eabdfa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% текстов ≤ 278 токенов\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(tokenizer(t, truncation=False)['input_ids']) for t in texts]\n",
    "print(f\"95% текстов ≤ {np.percentile(lengths, 95):.0f} токенов\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
