{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb3e3354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      " tensor([[12, 56, 78, 90, 43, 22, 11,  8],\n",
      "        [ 5,  9, 12, 34, 23, 76, 89,  0],\n",
      "        [ 2, 45, 23,  0,  0,  0,  0,  0]])\n",
      "Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "Labels: [tensor(0), tensor(0), tensor(1)]\n",
      "Lengths: [8, 7, 3]\n",
      "========================================\n",
      "Texts:\n",
      " tensor([[ 3,  7,  8,  5,  1,  4,  6, 10, 11],\n",
      "        [65, 12, 99, 54,  0,  0,  0,  0,  0]])\n",
      "Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "Labels: [tensor(1), tensor(2)]\n",
      "Lengths: [9, 4]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "texts = [\n",
    "    [5, 9, 12, 34, 23, 76, 89],          # len = 7\n",
    "    [2, 45, 23],                         # len = 3\n",
    "    [12, 56, 78, 90, 43, 22, 11, 8],     # len = 8\n",
    "    [65, 12, 99, 54],                    # len = 4\n",
    "    [3, 7, 8, 5, 1, 4, 6, 10, 11],       # len = 9\n",
    "]\n",
    "labels = [0, 1, 0, 2, 1]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "                'label': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    " # посчитайте длины текстов в батче\n",
    "    l = [len(c) for c in texts]\n",
    "    all = [(l[i], texts[i], labels[i]) for i in range(len(texts))]\n",
    "                 \n",
    "    # отсортируйте тексты и классы по убыванию длины текстов\n",
    "    all = sorted(all, key = lambda x: x[0], reverse=True)\n",
    "    lengths = [c[0] for c in all]\n",
    "    texts = [c[1] for c in all]\n",
    "    labels = [c[2] for c in all]\n",
    "\n",
    "    # дополните тексты пэддингом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    \n",
    "    # посчитайте маску для батча\n",
    "    masks = (padded_texts != 0).long()\n",
    "\n",
    "    return {\n",
    "        'texts': padded_texts,\n",
    "        'masks': masks,\n",
    "        'labels': labels,\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "\n",
    "\n",
    "# создайте объект dataloader с batch_size=3 и реализованным collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"Texts:\\n\", batch['texts'])\n",
    "    print(\"Masks:\\n\", batch['masks'])\n",
    "    print(\"Labels:\", batch['labels'])\n",
    "    print(\"Lengths:\", batch['lengths'])\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f176fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0])\n",
      "{'texts': tensor([[ 5,  9, 12,  0],\n",
      "        [ 2, 45, 23, 11]]), 'masks': tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]]), 'labels': tensor([1, 0])}\n",
      "tensor([1])\n",
      "{'texts': tensor([[12]]), 'masks': tensor([[1]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "# имортируем нужные библиотеки\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# создаем датасет, наследуясь от класса Dataset из PyTorch\n",
    "class RawDataset(Dataset):\n",
    "    # в конструкторе просто сохраняем тексты и классы\n",
    "    def __init__(self, texts, labels, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # возвращаем размер датасета (кол-во текстов)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # возвращаем текст и его класс\n",
    "        # для текста ограничиваем длину\n",
    "        # не делаем никаких доп. преобразований как padding и masking\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx][:self.max_len], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # список текстов и классов из батча\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    print(labels)\n",
    "    # дополняем тексты в батче padding'ом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    # считаем маски\n",
    "    masks = (padded_texts != 0).long()\n",
    "    # возвращаем преобразованный батч\n",
    "    return {\n",
    "        'texts': padded_texts,\n",
    "        'masks': masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "texts = [[5, 9, 12], [2, 45, 23, 11], [12]]\n",
    "labels = [1, 0, 1]\n",
    "max_len = 5\n",
    "\n",
    "# создаем объект датасета\n",
    "dataset = RawDataset(texts, labels, max_len)\n",
    "\n",
    "# пользуемся готовым даталоадером из PyTorch, но с кастомной функцией collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ffd95e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': tensor([[ 5,  9, 12,  0],\n",
      "        [ 2, 45, 23, 11]]), 'masks': tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]]), 'labels': tensor([1, 0])}\n",
      "{'texts': tensor([[12]]), 'masks': tensor([[1]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "# имортируем нужные библиотеки\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# создаем датасет, наследуясь от класса Dataset из PyTorch\n",
    "class RawDataset(Dataset):\n",
    "    # в конструкторе просто сохраняем тексты и классы\n",
    "    def __init__(self, texts, labels, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # возвращаем размер датасета (кол-во текстов)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # возвращаем текст и его класс\n",
    "        # для текста ограничиваем длину\n",
    "        # не делаем никаких доп. преобразований как padding и masking\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx][:self.max_len], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # список текстов и классов из батча\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "\n",
    "    # дополняем тексты в батче padding'ом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # считаем маски\n",
    "    masks = (padded_texts != 0).long()\n",
    "\n",
    "    # возвращаем преобразованный батч\n",
    "    return {\n",
    "        'texts': padded_texts,\n",
    "        'masks': masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "texts = [[5, 9, 12], [2, 45, 23, 11], [12]]\n",
    "labels = [1, 0, 1]\n",
    "max_len = 5\n",
    "\n",
    "# создаем объект датасета\n",
    "dataset = RawDataset(texts, labels, max_len)\n",
    "\n",
    "# пользуемся готовым даталоадером из PyTorch, но с кастомной функцией collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b5c9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      " tensor([[12, 56, 78, 90, 43, 22, 11,  8],\n",
      "        [ 5,  9, 12, 34, 23, 76, 89,  0],\n",
      "        [ 2, 45, 23,  0,  0,  0,  0,  0]])\n",
      "Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "Labels: [tensor(0), tensor(0), tensor(1)]\n",
      "Lengths: [8, 7, 3]\n",
      "========================================\n",
      "Texts:\n",
      " tensor([[ 3,  7,  8,  5,  1,  4,  6, 10, 11],\n",
      "        [65, 12, 99, 54,  0,  0,  0,  0,  0]])\n",
      "Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "Labels: [tensor(1), tensor(2)]\n",
      "Lengths: [9, 4]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "texts = [\n",
    "    [5, 9, 12, 34, 23, 76, 89],          # len = 7\n",
    "    [2, 45, 23],                         # len = 3\n",
    "    [12, 56, 78, 90, 43, 22, 11, 8],     # len = 8\n",
    "    [65, 12, 99, 54],                    # len = 4\n",
    "    [3, 7, 8, 5, 1, 4, 6, 10, 11],       # len = 9\n",
    "]\n",
    "labels = [0, 1, 0, 2, 1]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "                'label': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    " # посчитайте длины текстов в батче\n",
    "    l = [len(c) for c in texts]\n",
    "    all = [(l[i], texts[i], labels[i]) for i in range(len(texts))]\n",
    "                 \n",
    "    # отсортируйте тексты и классы по убыванию длины текстов\n",
    "    all = sorted(all, key = lambda x: x[0], reverse=True)\n",
    "    lengths = [c[0] for c in all]\n",
    "    texts = [c[1] for c in all]\n",
    "    labels = [c[2] for c in all]\n",
    "\n",
    "    # дополните тексты пэддингом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    \n",
    "    # посчитайте маску для батча\n",
    "    masks = (padded_texts != 0).long()\n",
    "\n",
    "    return {\n",
    "        'texts': padded_texts,\n",
    "        'masks': masks,\n",
    "        'labels': labels,\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "\n",
    "\n",
    "# создайте объект dataloader с batch_size=3 и реализованным collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"Texts:\\n\", batch['texts'])\n",
    "    print(\"Masks:\\n\", batch['masks'])\n",
    "    print(\"Labels:\", batch['labels'])\n",
    "    print(\"Lengths:\", batch['lengths'])\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "texts = [\n",
    "    [5, 9, 12, 34, 23, 76, 89],          # len = 7\n",
    "    [2, 45, 23],                         # len = 3\n",
    "    [12, 56, 78, 90, 43, 22, 11, 8],     # len = 8\n",
    "    [65, 12, 99, 54],                    # len = 4\n",
    "    [3, 7, 8, 5, 1, 4, 6, 10, 11],       # len = 9\n",
    "]\n",
    "\n",
    "\n",
    "labels = [0, 1, 0, 2, 1]\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "                'label': torch.tensor(self.labels[idx], dtype=torch.long)}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    # посчитайте длины текстов в батче\n",
    "    lengths = torch.tensor([len(c) for c in texts])\n",
    "    lengths, ind_sort = lengths.sort(descending=True)\n",
    "    ind_sort = ind_sort.tolist()\n",
    "\n",
    "\n",
    "    # отсортируйте тексты и классы по убыванию длины текстов\n",
    "    texts = [texts[i] for i in ind_sort]\n",
    "    labels = torch.stack([labels[i] for i in ind_sort])\n",
    "\n",
    "    # дополните тексты пэддингом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    # посчитайте маску для батча\n",
    "    masks = (padded_texts != 0).long()\n",
    "    \n",
    "    return {\n",
    "        'texts': padded_texts,     # (batch_size, max_len)\n",
    "        'masks': masks,            # (batch_size, max_len)\n",
    "        'labels': labels,          # (batch_size,)\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "\n",
    "\n",
    "# создайте объект dataloader с batch_size=3 и реализованным collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n",
    "\n",
    "# вывод результатов\n",
    "for batch in dataloader:\n",
    "    print(\"Texts:\\n\", batch['texts'])\n",
    "    print(\"Masks:\\n\", batch['masks'])\n",
    "    print(\"Labels:\", batch['labels'])\n",
    "    print(\"Lengths:\", batch['lengths'])\n",
    "    print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
